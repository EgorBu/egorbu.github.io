#cover
hi everybody
my name is Egor, i'm from source{d}, we are working in area of machine learning on source code
today i'm going to tell you about our experience in this area and what we know about it

TODO:
my presentation contains several  parts
the first one will be 

#ml
i want to tell you about couple of trends
first trend is machine learning - this family of algorithms became super famous in recent years because of bunch useful and succesful application

QUESTION - please raise your hands if you had experience with machine learning
- awesome, a lot of people

#github
second ingredient is code
amount of open source code is growing tremendously fast recent years.
tech giants, amateur and experienced developers have been publishing bunch of tools, libraries and so on. and there is github - awesome place to collaborate and publish code

QUESTION - please raise your hands if you used open source code, libraries, frameworks like tensorflow, pytorch and so on
- awesome, a lot of people

QUESTION - please raise your hands if you contributed to open source - pull request to existing projects or published your own
- you see - a lot of people among us did it

#5
and machine learning on source code appeared as natural combination of two ingredients that I mentioned

one more reason why it's important to apply it in this area. average developer may read several thousands, maybe millions line of code in dozens of projects during several years. 
machine learning algorithm can scale much better - process all world source code in several days - it's definetly possible nowadays 

#dragons-mloncode
in our company we decided to make abbreviation ml on code instead of machine learning on source code. it's shorter, easy to remember
and we try to use this hashtag to make it easy to search about something new in this area

in this are there is no only one task that everybody want to solve like imagenet
that's why machine learning on source code is like terra ingognito

#7
let me give you some examples of machine learning applicatation that has been done or possible to make them in near future

#8
code naturalness
what does it mean?
when we write code we try to give meaningful names to identifiers like variable names, class names and so on. so even if source code doesn't have all properties of natural language, still it has many of them. and we can use these properties to make code look better, to make it more readable and friendly for developers. let's go to the example

#9
QUESTION - can you quess the nae of the class?
excellent- yes, it's database

#10
we can predict the name of the class based on what it containes.
what does it give us?
better naming - if algorithms can "understand" (hand movements to show quotes) some kind of meaning of whats going on - than it's possible to suggest to change variable names to improve readability of your code
or you can detect bad naming in your code
and as a result the quality

because bad naming may lead (CLICK)

#11
it can lead to a lot of problems during collaboration with any other developer

#12
search for projects by meaning
let's imagine you have project A - that doesn't satisfy some of your requirements.
Or maybe satisfy but you want to check alternatives. But you don't know if any other projects like this exist.  - so it will be really painful to find such kind of similar projects. and you still don't know if you find all of them

in our company we made the project called vecino for similarity search based on meaning - in several words - we made efficient search for nearest neighbors based tfidf weighted embeddings of identifiers. We used word mover's distance to measure similarity between projects.
and now we can search for similar repositories based on "meaning" of identifiers.
you may find a lot of details at the link in the bottom-right corner

TODO: QR in the beginning

#excodesearch
how many times have you been trying to understand if something is possible to do with framework or library and couldn't find correct key words to search?
the idea of exploratory search is to make several iterations of search and go deeper to detatils until you find the correct snippet of code
we did some experimetns - but still there are bunch of work to do in this area - and it's really important feature for smart code assistant


#14
QUESTION how many of you copy  and pasted code from different projects or stackoverflow? please raise your hands

even here we can see a lot of hands.
imagine huge corporations that has thousands of developers.
and such kind of copy-paste became really huge problem there.
one of the reason is: your company may lost millions if somebody will sue you for copy-paste (like google for using oracle-like api)

compare to natural language in code you can search for similar code not only by text but you can use structure of code or names.

as result of similar code dedection we can create global graph of dependent projects, handle licencing issues efficiently, and it will make refactoring easier
for example if you have copy-pasted function in many different places of your code, you can find them easily with such kind of similar code detetion

and this problem is solved and released to github in our company and I will tell you about it a bit later

#nncomplete-example
everyone of us wants to have bot that will write code instead of you.
unfortunately it's not possible (until some limits)

it's like generative models trained on text. let's imagine you trained the model on book war and peace - algorithm may even write something meaningful in one sentence but after paragraph you will see that this generated text is insane.

but you still may have smart assistant that will allow you to write code faster.

 we did experiments with code completion based on recurent neural networks. it can definetly study some micropatterns but fails to write long code. so you can see at this example how

compare to all other code completion we don't suggest which method or name to use but which operator to use

and obviously it can be combined with standart code suggestions approaches to take in account context around to make better name suggestion and so on


#16
there are more than 400 programming languages in the world. and at least several style guides for each of them.
and it may vary from project to project - single quotes or double quotes? tab versus spaces and so on.
and obviuosly not every project has automatic style checker or explicit guide about style. so developers will spend a lot of time trying to fix style during PR or writing the code trying to figure out what is the proper style

and it's waste of many for huge corporation, and developers may become dissapointed because it's not really interesting work to fix the style

#17
to solve this task we did first step - identifier splitter - so you can write code without shift and next step should be - apply function that will automatically transform code into correct style learned from the project.

so you will save a lot of time for developers

#18
and there are much more possible applications
name suggestion, useless comment detection, api misusage and much more

#19
we found bunch of ideas in this great book - code as a crime scene.
we extended them with machine learning algorithms and it works great.
if you want to analyze the code i recommend you to read this book, it will give you a lot of initial insights

#hercules
here it's the example
you can create an embedding of files based on git history - which files are modied together and create embeddings from this information.
the same can be done for developers.
you create bipartite graph of contribution - that this two developers modified this file, function, class.
than use this as information for embedding algorithm
and you can find developers with similar background or files with similar meaning after such kind embedding is trained.

#pipeline
usually one of the biggest problems everybody meets when developing new algorithms - where is data?
let me explain th current situation for ml on code area

#22
there are several datasets available right now
* ghtorrent - all metadata from github except git repositories itself
* publich git archive - we published it during the last couple of monthes - it containes only git repositories that has more than 50 stars at github
it's around 180k repositories with more than 55 millions of files and 16 billions of lines in head revision
* github data in google big query - but be careful you can be charged for query

and you can collect some specific dataset by yourself using github api or rover & borges that we developed for this purpose

#23
a bit more information about public git archive that we published
we developed efficient storage format for data based on git structure
you may have common root in several projects and save a lot of space based on it.
there is csv index so you can select what you want to download.

#24
the next question - which tools do you need to process data?
like in nlp or image processing you may need some toolы

#mloncode-pyramid
in case of code you need quite a lot of them

the first bottom layer - how to discover available git repositroies?
and it can be done with rovers that I mentioned before - they can retrieve repository URL from multiple repository hosting providers

next - is downloading or cloning step. and it can be done by borgesю it will download them and store in efficient format called siva

each repository may contain hundreds and thousands of commits and branches. and you may want to work with only specific branch and commit - so you need checkout functionality

another point - repository may contain binary data like images, microsoft office files, - and you may want to filter out them. so you need filtering functionality 

QUESTION: another good question - what is the step required for parsing?
we need to know what the language code before we parse code
obviously to know which parser to use - so we need language classification

next step is parsing - extracting ast from code

and than you can start analysis.

so as you see you need a lot of steps before you can make proper analysis of code.

#26
let's go deeper and check what are the available tools for processing code

#27
I have already mentioned borges and rovers that solve problems at two bottom layers of pyramid.
and engine is the tool to solve all other except last one at scale

engine is an extension for apache spark that can  read siva files or repositories in a very scalable way to process huge amount of data

and there is a link to this tool at github

#28
and an example how it works
you import engine
then initialize it on siva files or repositories

then you may select repositories, the reference you want to process, commits and their content 
than you may classify languages and show table with blob, it's path and language

#ast
abstract syntax tree
to give you better understanding how ast looks here it's the example
it's a tree representation of code that appears in your pc memory after parsing the code 


TODO - BETTER WORDS

#30
there are several ways how ast can be extracted - 
based on regular expression like in pygments or highlight.js

antlr - another tool for language recognition - tool for parsing
todo - SEVERAL WORDS MORE


you can receive it after compilation

#31
or you can use bblfsh
it's a server for parsing source code at scale
multiple clients may send requests, bblfsh will spawn drivers according to workload

every language driver returns ast in uniformed format

and bblfsh is easy extandable for new languages

#32
what is universal format?
it has more or less uniform structure
THE RESULT OF PARSING of any programming language returned in a uniformed format - so you don't need to write adapter for every language to work with the this language.
as result you may apply the same algorithm to all programming languages without additional efforts

+ standard node types or roles like function or identifier among all languages

you may use xpath to search for specific nodes in ast
and it supports several ways to travers ast

you may see the example of parsing by bblfsh at dashboard

#33
and here it's an example how you can select identifiers from the python code using engine

#django
result of this query on django repository

so you can see blob ids, their paths and tokens that are contained there

#powerful
and it's extremly powerful tool
instead of combination of many  different custom tools for each specific language you may use only one to do almost everythong with code at scale

#36
next problem that appear when you train bunch of models and so on - how to store them?
we created modelforge library for it.
it helps to maintain the registry, the remote storage where all model files are stored in a structured, cataloged way.
it's based on flexible and modern binary format - advanced scientific data format

it's supports versioning that is important for real world application where model may become deprecated after some time

and more

#37
and finally - machine learning tools that you can already apply on your projects or any other projects

#38
hercules - it's library for gaining insights from your git repository history
it's based on go-git (go lang library for working with git), babelfsh and tensorflow
you can find the link to github

#39
right now you can prepare structural hotness - embeddings for functions, files, developers and so on
you can check which functions, classes, files were modified together, build co-occurrence matrix and train embeddings on it

and sentiments of the comments
you may think that there are no sentiments in source code but there are some
for example:
*sadly, we need to hide the rect from the documentation finder for now 
is negative and 
*Theano has a built-in optimization for logsumexp (...) so we can just write the expression directly
is positive


#40
and sourced ml library
it targets to conduct large-scale machine learning experiments on source code
it's based on sourced engine, modelforge and tensorflow

you can find link to the project here

and because we developed this tool for data scientists and machine learning engineers it's written in python

#41 
the problems that were solved already
splitting of identifiers - and it can be used for identifier embeddings and automatic style adjustment

efficient code similarity search with constant complexity for new sample

topic modeling of source code

and very important - embedding of identifiers

#42
let me tell you more details about identifier embeddings because it's very basic and important building block for many different algorithms like rnn, similarity search and so on


#43
what are the requirements for embeddings?
embeddings for similar objects should be closer to each other than distance to some other project

for example we have variable foo, than bar 
and some random variable integrate
so the distance between foo and bar should be smaller than the distance between foo and integrate

how can we measure distance?
for example as cosinus similariyt between two vectors
and it's based on scalar product of vectors divided by their norm

#44
how can we estimate scalar product?


#45
let's use this peace of code as an example for visualization of how embeddings are done

#46
the first necessary step is preprocessing of identifiers. because each of them may contain of several subidentifiers. 
and if you don't split number of identifiers will explode (for example right now in PGA around 60 millions of unique identifiers)

#47
so we are traversing AST and visit different visibility scopes on the way

so we know that all these identifiers appears together so we increase the count for them in co-occurrence matrix

#48
and this the visualization of connection among them
they look quite the same but let's go deeper

#49
so we go to the subtree or next visibility scope
and update co-occurrence matrix again



#50
so the visualization changed
let's continue

till #all2all
so we go deeper and update co-occurrence matrix always
and this is our final result that makes much more sense

#66
so after traversing of AST we receive incidence matrix
where number on intersection of row and column is number of these two identifiers appeared together


#pmi
we will use co-occurrence matrix for calculating pointwise mutual information

#68
and then we will use it for learning the representation for each identifier in dataset

#69
using stichastic gradient descent

#70
as in many real life applications we will have long tail distribution

#71
then we want to split into shards with respect to initial distribution for efficient parallelisation

#72
and swivel fits perfectly for our requirements
it scales with the size of vocabulary but not the size of dataset
it can be distributed horizontally using multi gpu and multi node environment

here you may find article and implementation in tensorflow

#73
and here some results of embeddings
nearest variable names to foo is

#74
it has property of analogy search but for source code

#75
and obviously you can use it for some kind of misspirnt correction



#76
let's summarize

#77
ml on code has bunch of application that can improve our experience of development
that can speed up developers work, make it easier and release developers from some stupid work and allow them to focus on something more interesting
bunch of data available
many tools were realesed
and community is growing around this area

#amloncode
one more thing
we organized awesome list to structure information and track the progress in this area about new applications, articls, datasets in this area, tools and so on

#79
thank you for your attention
you may find link to this presentation or use qr code
it was pleasure to give talk for you








